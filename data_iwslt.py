import os
import re
import xml.etree.ElementTree as ET
from torch.utils.data import Dataset

def read_xml(filepath):
    """è§£æ XML æ–‡ä»¶ï¼Œæå– <seg> æ®µè½æ–‡æœ¬"""
    tree = ET.parse(filepath)
    root = tree.getroot()
    texts = [seg.text.strip() for seg in root.iter("seg") if seg.text]
    return texts

def read_txt(filepath):
    """è¯»å– IWSLT2017 è®­ç»ƒé›†çº¯æ–‡æœ¬æ–‡ä»¶ï¼Œè·³è¿‡æ³¨é‡Šä¸æ ‡ç­¾"""
    texts = []
    with open(filepath, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            # è·³è¿‡ XML / HTML / æ³¨é‡Š / ç©ºè¡Œ
            if not line or line.startswith("<") or line.startswith("&"):
                continue
            texts.append(line)
    return texts

def load_iwslt_dataset(data_dir):
    """åŠ è½½ IWSLT2017 è‹±å¾·ç¿»è¯‘æ•°æ®é›†"""
    train_de = read_txt(os.path.join(data_dir, "train.tags.en-de.de"))
    train_en = read_txt(os.path.join(data_dir, "train.tags.en-de.en"))

    # ğŸ” è‡ªåŠ¨å¯¹é½å¥å­æ•°ï¼ˆæœ‰äº›ç‰ˆæœ¬ç•¥æœ‰å·®å¼‚ï¼‰
    min_len = min(len(train_de), len(train_en))
    train_de, train_en = train_de[:min_len], train_en[:min_len]

    dev_de = read_xml(os.path.join(data_dir, "IWSLT17.TED.dev2010.en-de.de.xml"))
    dev_en = read_xml(os.path.join(data_dir, "IWSLT17.TED.dev2010.en-de.en.xml"))

    test_de = read_xml(os.path.join(data_dir, "IWSLT17.TED.tst2010.en-de.de.xml"))
    test_en = read_xml(os.path.join(data_dir, "IWSLT17.TED.tst2010.en-de.en.xml"))

    assert len(dev_de) == len(dev_en)
    assert len(test_de) == len(test_en)

    dataset = {
        "train": [{"de": d, "en": e} for d, e in zip(train_de, train_en)],
        "validation": [{"de": d, "en": e} for d, e in zip(dev_de, dev_en)],
        "test": [{"de": d, "en": e} for d, e in zip(test_de, test_en)],
    }

    print("âœ… æˆåŠŸåŠ è½½ IWSLT2017 æ•°æ®é›†ï¼")
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_de)}")
    print(f"éªŒè¯æ ·æœ¬æ•°: {len(dev_de)}")
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_de)}")
    return dataset

class TranslationDataset(Dataset):
    def __init__(self, data, tokenizer=None, max_len=128):
        self.data = data
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        if self.tokenizer:
            src = self.tokenizer(
                item["de"],
                truncation=True,
                padding="max_length",
                max_length=self.max_len,
                return_tensors="pt",
            )
            tgt = self.tokenizer(
                item["en"],
                truncation=True,
                padding="max_length",
                max_length=self.max_len,
                return_tensors="pt",
            )
            return {"src": src, "tgt": tgt}
        return item


if __name__ == "__main__":
    dataset = load_iwslt_dataset("data")
    print(dataset["train"][0])
